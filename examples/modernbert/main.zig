const flags = @import("tigerbeetle/flags");
const std = @import("std");
const zml = @import("zml");
const asynk = @import("async");
const log = std.log.scoped(.modernbert);
const Tensor = zml.Tensor;
const modernbert = @import("modernbert.zig");

// set this to false to disable the verbose logging
// const show_mlir = true;
// pub const std_options = .{
//     .log_level = .warn,
//     .log_scope_levels = &[_]std.log.ScopeLevel{
//         .{ .scope = .zml_module, .level = if (show_mlir) .debug else .warn },
//         .{ .scope = .modernbert, .level = .info },
//     },
// };

// pub const max_seq_len = 64; // 8192

// TODO: The softmax function was generated by AI. Should double check
fn softmax(logits: []f32) void {
    // Find max for numerical stability
    var max_logit: f32 = -std.math.inf(f32);
    for (logits) |logit| {
        max_logit = @max(max_logit, logit);
    }

    // Compute exp(logits - max_logit) and sum
    var sum: f32 = 0.0;
    for (logits) |*logit| {
        logit.* = @exp(logit.* - max_logit);
        sum += logit.*;
    }

    // Normalize
    if (sum > 0) {
        for (logits) |*logit| {
            logit.* = logit.* / sum;
        }
    }
}

// fill-mask pipeline
// ref: https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/fill_mask.py
pub fn unmask(
    bert: modernbert.ModernBertForMaskedLM,
    mod: zml.ModuleExe(modernbert.ModernBertForMaskedLM.forward),
    tokenizer: zml.tokenizer.Tokenizer,
    allocator: std.mem.Allocator,
    text: []const u8,
    top_k: usize,
) !void {
    _ = top_k; // autofix
    _ = bert; // autofix

    // tokenize input text
    const tokens = try tokenizer.encode(allocator, text, .{});
    defer allocator.free(tokens);

    // find "[MASK]" positions
    var mask_positions = std.ArrayList(usize).init(allocator);
    defer mask_positions.deinit();

    for (tokens, 0..) |token, i| {
        if (token == tokenizer.special_tokens.mask) {
            try mask_positions.append(i);
        }
    }

    if (mask_positions.items.len == 0) {
        log.err("Missing mask in input text : {s}", .{text});
        return error.InvalidInput;
    }

    // prepare input tensors
    const max_seq_len = 64;
    var input_ids = try allocator.alloc(i64, max_seq_len);
    var attention_mask = try allocator.alloc(i64, max_seq_len);
    defer allocator.free(input_ids);
    defer allocator.free(attention_mask);

    // init all values to 0
    @memset(input_ids, 0);
    @memset(attention_mask, 0);

    // fill with tokens and set mask to 1
    for (tokens, 0..) |token, i| {
        input_ids[i] = @intCast(token);
        attention_mask[i] = 1;
    }

    // create input tensors
    const input_shape = zml.Shape.init(.{ .b = 1, .s = max_seq_len }, .i64);
    const input_ids_tensor = try zml.Buffer.fromSlice(mod.platform(), input_shape.dims(), input_ids);
    defer input_ids_tensor.deinit();
    const attention_mask_tensor = try zml.Buffer.fromSlice(mod.platform(), input_shape.dims(), attention_mask);
    defer attention_mask_tensor.deinit();

    // forward
    const outputs: zml.Buffer = mod.call(.{ input_ids_tensor, attention_mask_tensor });
    defer outputs.deinit();

    // logits
    var outputs_buffer = try outputs.toHostAlloc(allocator);
    defer outputs_buffer.deinit(allocator);

    const batch_size = 1;
    const vocab_size = tokenizer.tokens.len;
    const seq_len = max_seq_len;

    const mask_pos = mask_positions.items[0];
    const base_offset = mask_pos * vocab_size;

    log.info("Dimensions:", .{});
    log.info("\tbatch_size: {}, seq_len: {}, vocab_size: {}", .{ batch_size, seq_len, vocab_size });
    log.info("\tmask_position: {}", .{mask_pos});
    log.info("\tbase_offset: {} = {} * {}", .{ base_offset, mask_pos, vocab_size });
    log.info("\toutput_buffer size: {}", .{outputs_buffer.count()});

    const total_elements = outputs_buffer.count() / @sizeOf(f32);
    if (base_offset >= total_elements) {
        log.err("Invalid offset: {} >= {}", .{ base_offset, total_elements });
        return error.InvalidOffset;
    }

    const logits = try allocator.alloc(f32, vocab_size);
    defer allocator.free(logits);

    // copy is useless
    const raw_logits = @as([*]const f32, @ptrCast(@alignCast(outputs_buffer.data.ptr)));
    @memcpy(logits, raw_logits[base_offset..][0..vocab_size]);

    log.info("First logits:", .{});
    for (0..5) |i| {
        if (i < vocab_size) {
            var token_buffer = [_]u32{@intCast(i)};
            var token_text = std.ArrayList(u8).init(allocator);
            defer token_text.deinit();
            try tokenizer.decodeWithOpts(&token_text, &token_buffer, .{});
            log.info("\tlogit[{}] = {d:.6} (token: '{s}')", .{ i, logits[i], token_text.items });
        }
    }

    var indices = try allocator.alloc(usize, vocab_size);
    defer allocator.free(indices);
    for (0..vocab_size) |i| {
        indices[i] = i;
    }

    // sort desc
    const Context = struct {
        logits: []const f32,
        pub fn lessThan(ctx: @This(), a: usize, b: usize) bool {
            return ctx.logits[a] > ctx.logits[b];
        }
    };
    std.mem.sort(usize, indices, Context{ .logits = logits }, Context.lessThan);

    softmax(logits);

    log.info("Top predictions:", .{});
    for (indices[0..5]) |token_id| {
        if (token_id >= vocab_size) continue; // TODO: remove

        var token_buffer = [_]u32{@intCast(token_id)};
        var token_text = std.ArrayList(u8).init(allocator);
        defer token_text.deinit();
        try tokenizer.decodeWithOpts(&token_text, &token_buffer, .{});

        log.info("\tscore: {d:.6}, word: '{s}', token: {}", .{ logits[token_id], token_text.items, token_id });
    }
}

pub fn main() !void {
    try asynk.AsyncThread.main(std.heap.c_allocator, asyncMain);
}

pub fn asyncMain() !void {
    const CliArgs = struct {
        model: []const u8,
        tokenizer: ?[]const u8 = null,
        num_attention_heads: ?i64 = null,
        text: ?[]const u8 = null,
        create_options: []const u8 = "{}",
    };

    const allocator = std.heap.c_allocator;

    const tmp = try std.fs.openDirAbsolute("/tmp", .{});
    try tmp.makePath("zml/modernbert/cache");

    var context = try zml.Context.init();
    defer context.deinit();

    const compilation_options = zml.CompilationOptions{
        .xla_dump_to = "/tmp/zml/modernbert",
        .sharding_enabled = true,
    };

    var args = std.process.args();
    const cli_args = flags.parse(&args, CliArgs);
    const model_file = cli_args.model;

    var arena_state = std.heap.ArenaAllocator.init(allocator);
    defer arena_state.deinit();
    const model_arena = arena_state.allocator();

    const create_opts = try std.json.parseFromSliceLeaky(zml.Platform.CreateOptions, model_arena, cli_args.create_options, .{});
    const platform = context.autoPlatform(create_opts).withCompilationOptions(compilation_options);
    context.printAvailablePlatforms(platform);

    log.info("Model file: {s}", .{model_file});

    var ts = try zml.aio.detectFormatAndOpen(allocator, model_file);
    defer ts.deinit();

    const num_attention_heads = cli_args.num_attention_heads orelse ts.metadata("num_heads", .int) orelse @panic("--num-attention-heads is required for this model");
    const modernbert_options = modernbert.ModernBertOptions{
        .num_attention_heads = num_attention_heads,
        .tie_word_embeddings = true, // TODO: from cli_args
    };
    var modern_bert_for_masked_lm = try zml.aio.populateModel(modernbert.ModernBertForMaskedLM, model_arena, ts);

    if (cli_args.tokenizer == null) {
        log.err("Model doesn't have an embbedded tokenizer, please provide a path to a tokenizer.", .{});
        @panic("No tokenizer provided");
    }

    log.info("ModernBERT decoder before init : {?}", .{modern_bert_for_masked_lm.decoder});
    modern_bert_for_masked_lm.init(modernbert_options);
    log.info("ModernBERT decoder after init : {?}", .{modern_bert_for_masked_lm.decoder});

    log.info("✅\tParsed ModernBERT config: {}", .{modernbert_options});

    if (cli_args.tokenizer == null) {
        log.err("Model doesn't have an embbedded tokenizer, please provide a path to a tokenizer.", .{});
        @panic("No tokenizer provided");
    }
    const tokenizer_path = cli_args.tokenizer orelse cli_args.model;
    log.info("\tLoading tokenizer from {s}", .{tokenizer_path});
    var tokenizer = try zml.aio.detectFormatAndLoadTokenizer(allocator, tokenizer_path);
    log.info("✅\tLoaded tokenizer from {s}", .{tokenizer_path});
    defer tokenizer.deinit();

    // Prepare shapes for compilation
    // Note: we compile the model without a batching dimension ?
    const max_seq_len = 64;
    const input_shape = zml.Shape.init(.{ .b = 1, .s = max_seq_len }, .i64);
    const attention_mask_shape = input_shape;

    // Compile the model
    log.info("\tCompiling model...", .{});
    var start = try std.time.Timer.start();
    var fut_mod = try asynk.asyncc(zml.compile, .{
        allocator,
        modernbert.ModernBertForMaskedLM.forward,
        .{modernbert_options},
        .{ input_shape, attention_mask_shape },
        ts,
        platform,
    });

    // Load weights
    log.info("\tLoading ModernBERT weights from {s}...", .{cli_args.model});
    var bert_weights = try zml.aio.loadBuffers(modernbert.ModernBertForMaskedLM, .{modernbert_options}, ts, model_arena, platform);
    defer zml.aio.unloadBuffers(&bert_weights);
    log.info("✅\tLoaded weights in {d}ms", .{start.read() / std.time.ns_per_ms});

    var bert_module = (try fut_mod.awaitt()).prepare(bert_weights);
    defer bert_module.deinit();
    log.info("✅\tCompiled model in {d}ms", .{start.read() / std.time.ns_per_ms});

    const text = cli_args.text orelse "Zig is the [MASK] programming language.";
    log.info("\tInput text: {s}", .{text});

    try unmask(modern_bert_for_masked_lm, bert_module, tokenizer, allocator, text, 3);
}
